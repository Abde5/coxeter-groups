\textbf{Warnig: hypothesis reduced in the exchange property!}


\begin{theorem} [Matsumoto]
Let $W$ be a group and $S \subset W$ a finite subset of generators of $W$ of order $2$. Then the following assertions are equivalent:
\begin{itemize}
\item $(i)$ $(W, S)$ is a Coxeter system.
\item $(ii)$ $(W, S)$ satisfies the exchange property.
\item $(iii)$ $(W,S)$ satisfies the deletion property.
\end{itemize}
\end{theorem}
\begin{proof}
$(i) \Rightarrow (ii)$. This implication has already been shown above.

$(ii) \Rightarrow (iii)$. Let $w= s_1 \ldots s_k$ such that $\ell(w) < k$. Let $i$ be maximal such that $s_i s_{i+1} \ldots s_k$ is not reduced (i.e. $s_{i+1} \ldots s_k$ is reduced). We have $\ell (s_i s_{i+1} \ldots s_k) \le k- i = \ell (s_{i+1} \ldots s_k)$. Now, using exchange property, we obtain $s_i s_{i+1} \ldots s_k = s_{i+1} \ldots \hat{s}_j \ldots s_k$ for some $i+1 \le j \le k$. Therefore, $w = s_1 \ldots s_{i-1} s_i s_{i+1} \ldots s_k = s_1 \ldots s_{i-1} \hat{s}_i s_{i+1} \ldots \hat{s}_j \ldots s_k$ and we have the result (let us note that this implication remains true for weaker hypothesis since we did not use the fact that $S$ is of order $2$).

$(iii) \Rightarrow (ii)$. Let $w= s_1 \ldots s_k$, $k = \ell (w)$, $s\in S$, such that $\ell (s w) = \ell (s s_1 \ldots s_k) \le \ell (w) = \ell (s_1 \ldots s_k ) = k$. So $s s_1 \ldots s_k$ is not reduced. We can apply the deletion property. Suppose that $s w = s s_1 \ldots \hat{s}_i \ldots \hat{s}_j \ldots s_k $ (but $\ell (sw) \le k-1 < l(w)$). So $s s w = s s s_i \ldots \hat{s}_i \ldots {\hat{s}_j} \ldots s_k$. This leads to $\ell (s_1 \ldots ... \hat{s}_i \ldots \hat{s}_j \ldots s_k ) < k$, which is a contradiction, so this case has to be excluded. Hence, we have $s w = \hat{s} s_1 \ldots \hat{s}_i  \ldots s_k $.

$(ii) \Rightarrow (i)$. Using $(ii) \Rightarrow (iii)$, we can assume both $(ii)$ and $(iii)$. Define $m(s,s') = \text{order of $ss'$ in $W$, for all $s,s' \in S$}$. Let $(\tilde{W}, S)$ be the Coxeter group associated to $m$. Clearly, $\phi: \tilde{W} \mapsto W, s \to s$ is a surjective homomorphism. We need to show that $\phi$ is also injective. Let $s_1 \ldots s_m = e$ in $W$. By the deletion property, $m$ is even, say $m=2k$. So we can write our relation on the form
\begin{equation}
s_1 \ldots s_k = s'_1 \ldots s_k'
\label{relation}
\end{equation} where $s_1' = s_{2k}$, $\ldots$ $s_k' = s_{k+1}$. We must now prove that \eqref{relation} is a consequence of the pairwise relations $(ss')^{m(s,s')} = e$. The proof is done by induction on $k$, the case $k = 1$ being trivially correct.
\begin{itemize}
\item \underline{Case 1:} Suppose $w := s_1 \ldots s_k$ is not reduced. By deletion property, there exists a position $1 \le i <k$ such that $s_{i+1} s_{i+2} \ldots s_k$ is reduced but $s_i s_{i+1} s_{i+2} ...s_k$ is not. By the exchange property, we then have that $s_{i+1} s_{i+2} \ldots s_k = s_i s_{i+1} \ldots \hat{s}_j \ldots s_k$ for some $i < j \le k$. This relation is of length $<2k$ and hence fine. Plugging this result into \eqref{relation} gives $s_1 \ldots s_i s_i s_{i+1} \ldots \hat{s}_j \ldots s_k = s_1' s_2' \ldots s'_k$. The factor $s_i s_i$ can be deleted, leaving a relation of length $<2k$. Hence the relation \eqref{relation} is fine.
\item \underline{Case 2:} Suppose $w = s_1 \ldots s_k$ is reduced, $k = \ell (w)$. We can assume that $s_1 \neq s_1'$ (otherwise the relation \eqref{relation} is equivalent to a shorter relation). We have $\ell (s_1' s_1 s_2 \ldots s_k) = \ell (s_1' s_1' s_2' \ldots s_k') = \ell( s_2' \ldots s_k')  \le k-1 < \ell (s_1 \ldots s_k)$. Using exchange property, we have $s_1' s_1 \ldots s_k = s_1 \ldots \hat{s}_i \ldots s_k$ for some $i$. Hence, $s_1 \ldots \hat{s}_i \ldots s_k = s_2' \ldots s'_k$.

If $i<k$, then $s_1' s_1 s_2 \ldots s_{k-1} = s_1 \ldots \hat{s}_i \ldots s_{k-1}$. So $s_1' s_1 s_2 \ldots s_{k-1} s_k = s_1 \ldots \hat{s}_i \ldots s_{k-1} s_k$. Hence, $s_1' s_1 \ldots s_k = s_2' \ldots s_k$ is a consequence of Coxeter relations.

If $i=k$, we have to work a little bit harder. We have $s_1' s_1 \ldots s_{k-1} = s_1' s_2' \ldots s_k' $. Thus it will suffice to show that $s_1 s_1 \ldots s_{k-1} = s_1 s_2 \ldots s_k$ is a consequence of Coxeter relations. Applying recursively the rule, we have $s_1 s_1' s_1 \ldots s_{k-2} = s'_1 s_1 \ldots s_{k-1}$ $\Rightarrow$ $s_1' s_1 s'_1 s_1 \ldots s_{k-3} = s_1 s_1' s_1 \ldots s_{k-2}$ $\Rightarrow$ $\ldots$ Thus in the end, the question will be reduced to the relation $s_1 s_1' s_1 s_1' \ldots = s_1' s_1 s_1' s_1 \ldots$, which is of course a consequence of the Coxeter relation $(s_1 s_1')^{m(s,s')}= e$.


\end{itemize}

\end{proof}


\begin{example}
The group $S_n$ can be generated by transpositions, which are order $2$ elements. Using the above theorem, we conclude that $S_n$ is actually a Coxeter group.
\end{example}

\section{Geometric representation}

Let $(W, S)$ be a Coxeter system, $S = \{s_1, \ldots s_n \}$, $m$ the associated Coxeter matrix. We write $m_{ij} = m(s_i, s_j)$. Let $V$ be a $\mathbb{R}$-vector space of dimension $n$, with a basis $\alpha_1, \ldots , \alpha_n$. We consider the symmetric bilinear map
\begin{equation}
\langle \cdot, \cdot \rangle : V \times V \mapsto \mathbb{R}
\end{equation} defined through
\begin{equation}
\langle \alpha_i , \alpha_j \rangle = \left \{
\begin{array}{c @{} c}
    &- \cos \left(\frac{\pi}{m_{ij}} \right) \quad \text{if } m_{ij} < + \infty \\
    &-1 \quad ~~~~~~~~~~~~~\text{if } m_{ij} = + \infty \\
\end{array}
\right.
\end{equation} Not that $\langle \cdot, \cdot \rangle$ is not positive definite in general.

\begin{proposition}
The following map extends to a homomorphism:
\begin{equation}
W \mapsto GL(V), s_i \to \sigma_i
\end{equation} where $\sigma_i :v \to  v - 2 \langle v, \alpha_i \rangle \alpha_i$.
\end{proposition}

\begin{remark}
We have $\sigma_i (\alpha_i) = \alpha_i - 2 \langle \alpha_i, \alpha_i \rangle \alpha_i = - \alpha_i$. Thus, if $v\in V$ is such that $\langle v, \alpha_i \rangle = 0$, then $\sigma_i (v) = v$. Therefore, if $\langle \cdot, \cdot \rangle$ was positive definite, $\sigma_i$ would be interpreted as a reflexion through the hyperplane orthogonal to $\alpha_i$.
\end{remark}

\begin{proof}
First, let us show that $\sigma_i$ is invertible for all $i$. We have $\sigma^2_i (v) = \sigma_i (v) - 2 \langle v, \alpha_i \rangle \sigma_i (\alpha_i) = v - 2  \langle v, \alpha_i \rangle \alpha_i + 2  \langle v, \alpha_i \rangle \alpha_i = v$.

Now, let us show that $(\sigma_i \sigma_j)^{m_{ij}} = Id_V$. For $i\neq j$, define $V_{ij} = \text{Span}_\mathbb{R} ( \{ \alpha_i , \alpha_j \} )$. Furthermore, $V_{ij}^\perp = \{ v\in V | \langle v, \alpha_i \rangle = 0, \langle v, \alpha_j \rangle = 0 \}$. Before proceeding, we show the following lemma:

\begin{lemma}
$V = V_{ij} \oplus V_{ij}^\perp$ if $m_{ij} < + \infty$.
\end{lemma}

\begin{proof}
Let $v \in V$. We want to find $\lambda_i, \lambda_j \in \mathbb{R}$ such that $\tilde{v} = \lambda_i \alpha_i + \lambda_j \alpha_j \in V_{ij}$ and $v - \tilde{v} \in V_{ij}^\perp$. We have
\begin{equation}
\begin{split}
\langle \tilde{v}, \alpha_i \rangle &= \lambda_i \langle \alpha_i , \alpha_i \rangle + \lambda_j \langle \alpha_i, \alpha_j \rangle \\
&= \lambda_i + C \lambda_j
\end{split}
\end{equation} where $C = \langle \alpha_i, \alpha_j \rangle = - \cos \left( \frac{\pi}{m_{ij}} \right)$. Furthermore,
\begin{equation}
\langle \tilde{v}, \alpha_j \rangle = \lambda_i C + \lambda_j
\end{equation} Since
\begin{equation}
\det \begin{pmatrix}
1 & C \\
C & 1
\end{pmatrix} = 1 - C^2 = 1 - \cos^2 \left( \frac{\pi}{m_{ij}} \right) \neq 0,
\end{equation} if $m_{ij} < + \infty$. Therefore, we can find unique $\lambda_i$ and $\lambda_j$ such that
\begin{equation}
\langle \tilde{v} , \alpha_i \rangle = \langle v, \alpha_i \rangle \quad \text{and} \quad \langle \tilde{v} , \alpha_j \rangle = \langle v, \alpha_j \rangle
\end{equation}
\end{proof}

Now let us come back to the proof of the proposition. Using the lemma, we have $v = \tilde{v} + (v- \tilde{v})$ such that $\langle v - \tilde{v} , \alpha_i \rangle = 0 = \langle v - \tilde{v}, \alpha_j \rangle = 0$. Hence $\sigma_i (v - \tilde{v}) = v- \tilde{v} - 2 \langle v - \tilde{v} , \alpha_i \rangle \alpha_i = v- \tilde{v}$ and $\sigma_j (v-\tilde{v}) = v - \tilde{v}$. In the basis $\{ \alpha_i, \alpha_j \}$ of $V_{ij}$, the matrix associated to $\sigma_i$ is given by
\begin{equation}
\begin{pmatrix}
-1 &-2 C \\
0 & 1
\end{pmatrix}
\end{equation} In fact, we have $\sigma_i (\alpha_i) = -\alpha_i$ and $\sigma_j (\alpha_j ) = \alpha_j - 2 C \alpha_i$. Similarly, the matrix associated to $\sigma_j$ is given by
\begin{equation}
\begin{pmatrix}
1 & 0 \\
-2C & -1
\end{pmatrix}
\end{equation} Therefore,
\begin{equation}
(\sigma_i) (\sigma_j) = \begin{pmatrix}
-1 &-2 C \\
0 & 1
\end{pmatrix} \begin{pmatrix}
1 & 0 \\
-2C & -1
\end{pmatrix} = \begin{pmatrix}
-1 + 4 C^2 & 2C \\
-2 C & -1
\end{pmatrix}
\end{equation} The characteristic polynomial $P$ of this matrix is given by $P(t) = t^2 - (-2 + 4 C^2 ) t + 1$.The roots are given by $t_\pm = \cos \left(\frac{2 \pi}{m_{ij}} \right) \pm i \sin \left( \frac{2 \pi}{m_{ij}} \right)$. This characterizes a rotation of $2 \pi / m_{ij}$. So, the order of $\sigma_i \sigma_j$ is given by $m_{ij}$. 

\end{proof}
